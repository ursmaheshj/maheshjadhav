<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>Create Your Own Dataset: A Step-by-Step Guide to Web Scraping using Selenium and Python for Data Collection and Analysis</h3> <p>The first and most important requirement in the field of data science is data. However, it’s not always possible to find the required datasets on websites like <a href="https://www.kaggle.com/ursmaheshj" rel="external nofollow noopener" target="_blank">Kaggle </a>or <a href="https://datahub.io/" rel="external nofollow noopener" target="_blank">Datahub</a>. If you’re lucky enough to work for a company, you might have access to data through APIs or Databases. But, most of the time data may not be readily available, and you need to figure out how to create your own datasets from scratch. In such cases, <strong>web scraping</strong> can be a useful tool to gather the required data effectively.</p> <p>Web scraping is a technique that extracts information from websites or web pages. This process uses automated software tools to scan the HTML of a website and gather various types of information, such as text, images, and links. The collected data can be examined, manipulated, and used for a variety of purposes like data mining, market research, and machine learning.</p> <p>Nowadays, there are many tools available to scrape websites, but some of them may not work if websites generate content dynamically using JavaScript. This is why <a href="https://selenium-python.readthedocs.io/" rel="external nofollow noopener" target="_blank"><strong>Selenium</strong></a><strong> </strong>is becoming increasingly popular as it offers customized configurations and behaves like a real person interacting with the website through a browser.</p> <h4>Problem statement:</h4> <blockquote>“Conduct an analysis of all the leading IT companies operating in India.<strong>”</strong> </blockquote> <p>There are various websites available from which you can obtain this data, but we will be using one of the best among them, <a href="https://www.ambitionbox.com/about-us" rel="external nofollow noopener" target="_blank"><strong><em>AmbitionBox</em></strong></a>. It is a career advisory and job search platform based in India. That also provides reviews and details about companies.</p> <p><strong>Let’s explore the <em>AmbitionBox </em>website using the browser.</strong></p> <ul><li><a href="https://www.ambitionbox.com/list-of-companies?IndustryName=it-services-and-consulting&amp;sort_by=popularity&amp;page=1" rel="external nofollow noopener" target="_blank">https://www.ambitionbox.com/list-of-companies?IndustryName=it-services-and-consulting&amp;sort_by=popularity&amp;page=1</a></li></ul> <figure><img alt="alternate image name" src="https://cdn-images-1.medium.com/proxy/1*0siZKUavTzjUZvKhcjztDg.png"><figcaption>AmbitionBox webpage</figcaption></figure> <p><strong>Observing the webpage carefully, let’s see how we can extract the data.</strong></p> <ol> <li>AmbitionBox provides details for approximately 14,588 companies, but for our purposes, we will be focusing mainly on IT companies.</li> <li>Each company is assigned a separate box, which contains all the details regarding the company.</li> <li>Each page on the website has around 30 boxes, meaning that one page represents data about 30 companies.</li> <li>As there are around 333 pages available, we will have access to data on around 9,990 companies.</li> </ol> <blockquote>You can get below code in the form of jupyter notebook <a href="https://github.com/ursmaheshj/WebScraping_with_Selenium_and_Python" rel="external nofollow noopener" target="_blank"><strong>here</strong></a> and the extracted dataset is available <a href="https://www.kaggle.com/datasets/ursmaheshj/top-it-companies-in-india" rel="external nofollow noopener" target="_blank"><strong>here</strong></a><strong>.</strong> </blockquote> <p><strong>Importing all required libraries</strong></p> <pre>import pandas as pd<br>from bs4 import BeautifulSoup<br>from selenium import webdriver<br>from selenium.webdriver.chrome.options import Options<br>from selenium.webdriver.chrome.service import Service<br>from time import sleep<br>import re</pre> <pre># Setting up the driver path where we have installed the<br># chromedriver<br><br>DRIVER_PATH = 'D:\Applications\Chromium\chromedriver_win32'</pre> <pre># Creating a Service object using the specified driver_path which<br># enables communication between the Selenium WebDriver and the<br># browser.</pre> <pre>s=Service(DRIVER_PATH)</pre> <h3>Extracting the data</h3> <p>In the next code block, we will use Selenium to iterate through all web pages and extract all the data using BeautifulSoup along with regular expressions. The extracted data will be saved into a dataset. Additionally, we will print the page number after reading each page so that if any issues occur, we can identify the page where the problem occurred.</p> <pre>%%time</pre> <pre># Create an empty Database</pre> <pre>company_df = pd.DataFrame(columns = [ 'name','rating','review','tags','company_type','headquarters','age','total_emp','about'<br>])</pre> <pre># setup options parameter for selenium session</pre> <pre>options = Options()<br>options.add_argument('--disable-blink-features=AutomationControlled')<br>options.add_argument("--window-size=1920,1200")</pre> <pre># Iterate for loop on all pages</pre> <pre>for p in range(1,334):<br>    try:<br>        driver = webdriver.Chrome(service=s,options=options)<br>        url=f"https://www.ambitionbox.com/list-of-companies?IndustryName=it-services-and-consulting&amp;sort_by=popularity&amp;page={p}"<br>        driver.get(url)<br>        soup = BeautifulSoup(driver.page_source,'html.parser')<br>        companies = soup.find_all("div", class_="company-content-wrapper")<br><br>        for i in companies:<br>            name = None<br>            rating = None<br>            review = None<br>            tags = None<br><br>            company_type = None<br>            headquarters = None<br>            age = None<br>            total_emp = None<br><br>            about = None<br><br>            try:<br><br>                name = i.find('h2',class_='company-name bold-title-l').text.strip()<br>                rating = i.find('p',class_='rating').text.strip()<br>                review = i.find('a',class_='review-count sbold-Labels').text.strip()<br>                tags = ','.join([t.find('a',class_='ab_chip body-medium').text.strip() for t in i.find('ul',class_='chips-block')])<br><br>                names = i.find('div',class_='company-basic-info')<br><br>                for n in names.find_all('p'):<br>                    try:<br>                        if n.find('i',class_='icon-domain'): company_type = n.text.strip()<br>                        if n.find('i',class_='icon-pin-drop'): headquarters = n.text.strip()<br>                        if n.find('i',class_='icon-access-time'): age = n.text.strip()<br>                        if n.find('i',class_='icon-supervisor-account'): total_emp = n.text.strip()<br>                    except:<br>                        continue<br><br>                about = i.find('p',class_='description').text.strip()<br>                <br>            except:<br>                continue<br>            finally:<br>                company_df.loc[len(company_df)] = [name,rating,review,tags,company_type,headquarters,age,total_emp,about]<br>                <br>        <br>        sleep(1)<br>    except Exception as e:<br>        print(f"Something went wrong: {e}")<br>    finally:<br>        driver.quit()<br>        print(p, end='|')<br>    <br>print('done')</pre> <pre>1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29<br>|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52|53|54|<br>55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78|79|80<br>|81|82|83|84|85|86|87|88|89|90|91|92|93|94|95|96|97|98|99|100|101|102|103|104<br>|105|106|107|108|109|110|111|112|113|114|115|116|117|118|119|120|121|122|123|<br>124|125|126|127|128|129|130|131|132|133|134|135|136|137|138|139|140|141|142|<br>143|144|145|146|147|148|149|150|151|152|153|154|155|156|157|158|159|160|161|<br>162|163|164|165|166|167|168|169|170|171|172|173|174|175|176|177|178|179|180|<br>181|182|183|184|185|186|187|188|189|190|191|192|193|194|195|196|197|198|199|<br>200|201|202|203|204|205|206|207|208|209|210|211|212|213|214|215|216|217|218|<br>219|220|221|222|223|224|225|226|227|228|229|230|231|232|233|234|235|236|237|<br>238|239|240|241|242|243|244|245|246|247|248|249|250|251|252|253|254|255|256|<br>257|258|259|260|261|262|263|264|265|266|267|268|269|270|271|272|273|274|275|<br>276|277|278|279|280|281|282|283|284|285|286|287|288|289|290|291|292|293|294|<br>295|296|297|298|299|300|301|302|303|304|305|306|307|308|309|310|311|312|313|<br>314|315|316|317|318|319|320|321|322|323|324|325|326|327|328|329|330|331|332|<br>333|done<br>CPU times: total: 31.3 s<br>Wall time: 44min 14s</pre> <p>Let’s explore the dataset created</p> <pre>company_df.head()</pre> <figure><img alt="png" src="https://cdn-images-1.medium.com/proxy/1*6iMKqHIakkINh_INYhADEg.png"></figure> <h3>Manipulating the Data</h3> <p>We have obtained the data, but it is not in a format that will be useful for analysis. Therefore, we need to clean the data and retain only the necessary information.<br>To ensure that we do not lose our original data in case anything goes wrong, we will perform these manipulations on a copy of the original dataset.</p> <pre>df = company_df.copy()</pre> <h4>1. review</h4> <p>This column represents numerical data, but Pandas has assigned it as a string. We will create a function that extracts only numerical data from the string.</p> <pre>def transform_review(data):<br>    try:<br>        if re.search(r'\d+k',data):<br>            return float(re.search(r'\d+\.\d+|\d+',data).group()) * 1000<br>        elif re.search(r'\d+',data):<br>            return float(re.search(r'\d+',data).group())<br>        else:<br>            return 0<br>    except:<br>        return 0</pre> <pre>df['review'] = df['review'].apply(transform_review)</pre> <pre>df.head(2)</pre> <figure><img alt="png" src="https://cdn-images-1.medium.com/proxy/1*WP_WM8Dg7tms90mHWyoN-A.png"></figure> <h4>2. headquarters</h4> <p>headquarters column consist of both the actual headquarters location and the number of offices that the company has. We can separate this information into two distinct columns, namely “headquarter” and “total_offices”.</p> <ul><li>headquarter</li></ul> <pre>def transform_headquarter(data):<br>    try:<br>        return data.split('+') [0].strip()<br>    except:<br>        return None</pre> <pre>df['headquarter'] = df['headquarters'].apply(transform_headquarter)</pre> <ul><li>total_offices</li></ul> <pre>def transform_total_offices(data):<br>    try:<br>        if re.search(r'\d+',data):<br>            data = data.split('+') [1].strip()<br>            return int(re.search(r'\d+',data).group()) + 1<br>        else:<br>            return 1<br>    except:<br>        return None</pre> <pre>df['total_offices'] = df['headquarters'].apply(transform_total_offices)</pre> <p>Now that we have successfully separated and added our columns, we can delete the unnecessary column ‘headquarters’.</p> <pre>del df['headquarters']</pre> <pre>df.head(2)</pre> <figure><img alt="png" src="https://cdn-images-1.medium.com/proxy/1*ULsHZtLvjpBi2VFkUzbaOw.png"></figure> <h4>3. age</h4> <p>This simply displays the age of the company from the year of its establishment. We can convert this numerical data into a date format and label it as ‘founded_year’, to gain a better understanding.</p> <pre>def transform_age(data):<br>    try:<br>        return int(pd.to_datetime('today').date().year - int(re.search(r'\d+',data).group()))<br>    except:<br>        return None</pre> <pre>df['founded_year'] = df['age'].apply(transform_age)</pre> <pre>del df['age']</pre> <pre>df.head(2)</pre> <figure><img alt="png" src="https://cdn-images-1.medium.com/proxy/1*O53YYJT8wb4uYP_YV1krsw.png"></figure> <h4>4. total_emp</h4> <p>Due to the mixture of text and numbers, the data is displayed as string values instead of numeric values. We will convert this data into numeric format. Please note that this is not the actual number of employees, but instead represents the minimum count of employees working in India.</p> <pre>def transform_total_emp(data):<br>    try:<br>        values = list(map(int,re.findall(r'\d+\.\d+|\d+',data)))<br>        units = re.findall(r'Lakh|k',data)<br>        if len(values)==0:<br>            return 0<br>        elif len(values)==1:<br>            if units[0] == 'Lakh':<br>                return values[0] * 100000<br>            elif units[0] == 'k':<br>                return values[0] * 1000<br>            else:<br>                return values[0]<br>        elif len(values)==2:<br>            if len(units)==0:<br>                return (values[0]+values[1])/2<br>            elif len(units)==1:<br>                return values[0]+(values[1]*1000)/2<br>            elif len(units)==2:<br>                if units[0] == units[1] == 'Lakh':<br>                    return (values[0]+values[1])/2 * 100000<br>                elif units[0] == units[1] == 'k':<br>                    return (values[0]+values[1])/2 * 1000<br>                else:<br>                    return ((values[0]*1000) + (values[1]*100000))/2 <br>        else:<br>            return 0<br>    except:<br>        return None</pre> <pre>df['total_emp'] = df['total_emp'].apply(transform_total_emp)</pre> <pre>df.head(2)</pre> <figure><img alt="png" src="https://cdn-images-1.medium.com/proxy/1*uCE49cd3GJuopu_IifEXRw.png"></figure> <pre>df.info()</pre> <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;<br>Int64Index: 9990 entries, 0 to 9989<br>Data columns (total 10 columns):<br> #   Column         Non-Null Count  Dtype  <br>---  ------         --------------  -----  <br> 0   name           9990 non-null   object <br> 1   rating         9990 non-null   object <br> 2   review         9990 non-null   float64<br> 3   tags           9990 non-null   object <br> 4   company_type   7344 non-null   object <br> 5   total_emp      9101 non-null   float64<br> 6   about          6699 non-null   object <br> 7   headquarter    9171 non-null   object <br> 8   total_offices  9166 non-null   float64<br> 9   founded_year   7675 non-null   float64<br>dtypes: float64(4), object(6)<br>memory usage: 858.5+ KB</pre> <p>Data has been cleaned successfully let’s correct the datatype of columns to store it in an efficient way</p> <pre>col_dtypes = {<br>                'name': str,<br>                'rating': int,<br>                'review': float,<br>                'tags': str,<br>                'company_type': str,<br>                'total_emp': float,<br>                'about': str,<br>                'headquarter': str,<br>                'total_offices': int,<br>                'founded_year': int,<br>            }<br>df = df.astype(col_dtypes,errors='ignore')</pre> <p><strong>Let’s check the entire dataset once again.</strong></p> <pre>df.head()</pre> <figure><img alt="png" src="https://cdn-images-1.medium.com/proxy/1*9s8vA5jCvQcZCOn0TV-dCQ.png"></figure> <p>We have all the necessary details, so now we can save the dataset in the desired format.</p> <pre>df.to_csv('AmbitionBox_Dataset.csv',index=False)</pre> <blockquote>You can get the original python jupyter notebook <a href="https://github.com/ursmaheshj/WebScraping_with_Selenium_and_Python" rel="external nofollow noopener" target="_blank"><strong><em>here</em></strong></a><strong><em> </em></strong>and the extracted dataset is available <a href="https://www.kaggle.com/datasets/ursmaheshj/top-it-companies-in-india" rel="external nofollow noopener" target="_blank"><strong><em>here</em></strong></a><strong>.</strong> </blockquote> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b4a6da73b71d" width="1" height="1" alt=""></p> </body></html>