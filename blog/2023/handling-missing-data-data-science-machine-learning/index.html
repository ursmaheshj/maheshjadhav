<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>How to Handle Missing Data | Machine learning | Data science</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ip2tYhtwCWVGo3Yen5fX7w.png"><figcaption>Handling Missing Data</figcaption></figure> <p>Handling missing data is one of the important tasks that a data scientist needs to be an expert in. When working on real-world projects, it is common to encounter missing data, and dealing with it requires careful planning to avoid bias and ensure accurate analysis and efficient model training.</p> <p>Missing data can be categorized into three types -</p> <p>1. <strong>MCAR</strong> — Missing completely at Random.<br>2. <strong>MAR</strong> — Missing at Random.<br>3. <strong>MNAR</strong> — Missing not at Random.</p> <p>There are mainly two ways to handle the missing data, either remove the missing values or impute the missing values based on some calculations.</p> <h3> <strong>Remove missing values</strong>:</h3> <p>The missing values are simply removed from the dataset. We use this method if the missing data is either MCAR or MAR and constitutes less than 5% of the total available data.</p> <blockquote> <strong>Advantages and Disadvantages:<br></strong>1. Easy to implement.<br>2. Preserves distribution if Data is MCAR.<br>3. Excluded data may contain important data and result in decreased model performance.<br>4. Unable to handle missing data in production.</blockquote> <ol> <li> <strong>Listwise deletion: </strong><br>Also known as CCA (Complete Case Analysis), It discards rows where values in any of the columns are missing.</li> <li> <strong>Pairwise deletion: </strong><br>Also known as ACA (Available Case Analysis), It minimizes the data loss compared to Listwise deletion by ignoring the missing values based on the correlation strength of the relationship between two variables.</li> <li> <strong>Dropping Column:</strong> <br>If any of the columns contain a large proportion of missing values and show no correlations with the target variable, then instead of removing rows, we can drop the entire column to simplify the dataset.</li> </ol> <h3> <strong>Imputing missing values</strong>:</h3> <p>In this technique, we predict the most appropriate value to replace missing values using different statistical methods.</p> <h4>Univariate Imputation:</h4> <p>Missing values are predicted based on the information available within that specific variable. Values are calculated with different methods based on the data type of that specific variable.</p> <blockquote> <strong>Advantages and Disadvantages:<br></strong>1. Overcomes data loss issue.<br>2. It may change shape of the distribution of data.<br>3. Change in covariance and correlation between data.<br>4. It may identify unnecessary outliers.</blockquote> <p><strong>Numerical Imputations:</strong></p> <ol> <li> <strong>Mean:<br></strong>Missing values are replaced with the mean of the column. Works best on normally distributed data</li> <li> <strong>Median:<br></strong>Missing values are replaced with the median of the column. Works best on skewed distribution.</li> <li> <strong>End of Distribution:<br></strong>missing values are replaced with far-end values or extreme far-end values. Based on distributions we use the below formulas.<br><em>Normal</em>: <em>(mean — 3σ) or (mean + 3σ)</em><br><em>Skewed</em>: <em>(Q1–1.5IQR) or (Q3 + 1.5IQR)</em> </li> <li> <strong>Random:<br></strong>Missing values are replaced with a random value selected from the available unique values present in that column. This technique helps preserve the variance and distribution of the data but can be memory-heavy during deployment.</li> </ol> <p><strong>Categorical Imputations:</strong></p> <ol> <li> <strong>Mode:<br></strong>Here, missing values are replaced with the most recurring value, which is the mode of that specific column.</li> <li> <strong>Arbitrary:<br></strong>If missing values are MNAR or constitute more than 5% of the total data then we can replace it with custom text like “MISSING”.</li> </ol> <h4>Multivariate Imputation:</h4> <p>In this technique, missing values are predicted based on the relationship between the data present in another column using different concepts like correlation, covariance, and Euclidean distance between two points.</p> <blockquote> <strong>Advantages and Disadvantages:<br></strong>1. Gives the most accurate predictions for missing data.<br>2. More no of calculations are required which may slow down the process.<br>3. Memory heavy in case of deployment on production.</blockquote> <ol> <li> <strong>KNN Imputer:<br></strong>Missing values are predicted with the help of the K-nearest neighbor algorithm using Euclidean distance. where <em>k</em> is the no. of nearest neighbors to be taken in the calculation.<br><em>Euclidean distance(x,y) = sqrt(weight * sq. distance from present coordinates) </em><br>where, <em>weight = Total no of coordinates / no of present coordinates</em> </li> <li> <strong>Iterative Imputer:<br></strong>Also known as MICE (Multivariate Imputation by Chained Equations), this technique helps to predict missing values using a machine learning (ML) model. The process involves filling in missing values using SimpleImputer with any chosen strategy. The missing feature column is designated as the output variable y, while the other feature columns are treated as input variables X. A regressor is fitted on (X, y) for known y values. Subsequently, the regressor is used to predict the missing values of y. This iterative process is performed for each feature, and repeated for a maximum of max_iter imputation rounds. The results obtained from the final imputation round are returned.</li> </ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a215056a07e3" width="1" height="1" alt=""></p> </body></html>